{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83e731b",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#005097; border:0' role=\"tab\" aria-controls=\"home\"><center>APRENDIZADO DE MÁQUINA (CIC1205) - Trabalho 2</center></h1>\n",
    "\n",
    "- Nome completo: MATHEUS SANTOS MELO\n",
    "- Matrícula: 2430148MCICMA\n",
    "- [Link para vídeo](https://cefetrjbr-my.sharepoint.com/:v:/g/personal/18888284710_cefet-rj_br/EVKdfxdq6epJjx2ns4rxYtIBLW-OZufNVc10ZvPnMGGFhw?e=x38xWU&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f65258",
   "metadata": {},
   "source": [
    "- Para esse trabalho, utilizei a versão do Python 3.12.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c199ea",
   "metadata": {},
   "source": [
    "## 0. Importação das bibliotecas para todas as questões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92130626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação de Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualização de Dados\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Algoritmos de Classificação e Regressão\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Pré-processamento e Transformações\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Avaliação de Modelos\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Utilitários\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# Para ignorar mensagens de aviso\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Parâmetro para utilizar no random_state por fins de reprodutibilidade\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76ad9c",
   "metadata": {},
   "source": [
    "## 1. Engenharia de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb58a809",
   "metadata": {},
   "source": [
    "- Para essa questão, irei criar alguns atributos derivados que façam sentido e depois irei analisá-los para saber se tem uma relação com a variável-alvo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d80530",
   "metadata": {},
   "source": [
    "#### Descrição dos dados (Fonte: [Kaggle](https://www.kaggle.com/datasets/shivam2503/diamonds)):\n",
    "\n",
    "- **price**: price in US dollars ($326--$18,823)\n",
    "\n",
    "- **carat**: weight of the diamond (0.2--5.01)\n",
    "\n",
    "- **cut**: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "\n",
    "- **color**: diamond colour, from J (worst) to D (best)\n",
    "\n",
    "- **clarity**: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "\n",
    "- **x**: length in mm (0--10.74)\n",
    "\n",
    "- **y**: width in mm (0--58.9)\n",
    "\n",
    "- **z**: depth in mm (0--31.8)\n",
    "\n",
    "- **depth**: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
    "\n",
    "- **table**: width of top of diamond relative to widest point (43--95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds = pd.read_csv('../data/diamonds.csv', index_col=0)\n",
    "diamonds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4d7a3",
   "metadata": {},
   "source": [
    "### Feature 1: Volume do diamante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082ead4",
   "metadata": {},
   "source": [
    "- Justificativa: O volume é uma medida física tridimensional, e pode representar melhor o \"corpo\" real do diamante, principalmente quando existem variações no formato que não são bem explicadas só pelo `carat`.\n",
    "\n",
    "- Variáveis utilizadas: `x`, `y`, `z` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f05e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds['volume'] = diamonds['x'] * diamonds['y'] * diamonds['z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sp.make_subplots(rows=1, cols=2, horizontal_spacing=0.1, subplot_titles=['Scatterplot', 'Heatmap'])\n",
    "\n",
    "# Primeiro subplot\n",
    "fig.add_trace(go.Scatter(x= diamonds['volume'], y=diamonds['price'], mode='markers'), row=1, col=1)\n",
    "\n",
    "# Segundo subplot\n",
    "fig.add_trace(go.Heatmap(z = diamonds[['volume', 'price']].corr(), zmin=-1, zmax=1, \n",
    "                         x = ['volume', 'price'], y = ['volume', 'price'], texttemplate=\"%{z:.3f}\", coloraxis=\"coloraxis\"), row=1, col=2)\n",
    "\n",
    "# Atualizando layout com rótulos dos eixos e exibição do gráfico\n",
    "fig.update_layout(xaxis1_title=\"volume\", yaxis1_title=\"price\", height=400, width=1250, coloraxis={'colorscale': 'Turbo', 'cmin': -1, 'cmax': 1}, \n",
    "                  title= 'Análise exploratória entre a feature criada e o preço')\n",
    "\n",
    "fig.show('png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4834579",
   "metadata": {},
   "source": [
    "- Podemos observar que o processo de Feature Engineering para criação da variável de volume tem uma correlação positiva não-linear alta em relação ao preço do diamante. Ou seja, quanto maior for o volume do diamante, maior o preço, o que faz sentido seguindo a lógica que o preço aumenta conforme o valor do diamanete. \n",
    "\n",
    "- O ponto interessante é que como foi imaginado, o valor do diamante aumenta de uma maneira exponencial de acordo com o seu volume, o que pode ser visto a partir do gráfico de dispersão acima.\n",
    "\n",
    "- Sendo assim, é uma boa variável para ser considerada para o modelo preditivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084832a",
   "metadata": {},
   "source": [
    "### Features 2 e 3: Quilate com transformação logarítmica e polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18c3ef",
   "metadata": {},
   "source": [
    "- Justificativa: A transformação logarítmica e polinomial de `carat` pode ajudar a linearizar sua relação com o preço, além de possivelmente reduzir o impacto de outliers e capturar eventuais efeitos não lineares do peso sobre o valor do diamante. Criarei as duas features pois minha intenção é testar essa abordagem de transformação tanto de forma logarítimica quanto polinomial e ver qual possui melhor relação com o target.\n",
    "\n",
    "- Variável utilizada: `carat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def apply_log(X):\n",
    "    return math.log(X, 10) \n",
    "\n",
    "def apply_poly(X):\n",
    "    return X**2\n",
    "\n",
    "diamonds['log_carat'] = diamonds['carat'].apply(lambda k: apply_log(k))\n",
    "diamonds['carat²'] = diamonds['carat'].apply(lambda k: apply_poly(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3061ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sp.make_subplots(rows=2, cols=2, horizontal_spacing=0.1, subplot_titles=['Scatterplot', 'Heatmap', 'Scatterplot', 'Heatmap'], vertical_spacing=0.25)\n",
    "\n",
    "# Primeiro subplot\n",
    "fig.add_trace(go.Scatter(x= diamonds['log_carat'], y=diamonds['price'], mode='markers'), row=1, col=1)\n",
    "\n",
    "# Segundo subplot\n",
    "fig.add_trace(go.Heatmap(z = diamonds[['log_carat', 'price']].corr('spearman'), zmin=-1, zmax=1, \n",
    "                         x = ['log_carat', 'price'], y = ['log_carat', 'price'], texttemplate=\"%{z:.3f}\", coloraxis=\"coloraxis\"), row=1, col=2)\n",
    "\n",
    "# Terceiro subplot\n",
    "fig.add_trace(go.Scatter(x= diamonds['carat²'], y=diamonds['price'], mode='markers'), row=2, col=1)\n",
    "\n",
    "# Quarto subplot\n",
    "fig.add_trace(go.Heatmap(z = diamonds[['carat²', 'price']].corr('spearman'), zmin=-1, zmax=1, \n",
    "                         x = ['carat²', 'price'], y = ['carat²', 'price'], texttemplate=\"%{z:.3f}\", coloraxis=\"coloraxis\"), row=2, col=2)\n",
    "\n",
    "# Atualizando layout com rótulos dos eixos e exibição do gráfico\n",
    "fig.update_layout(xaxis1_title=\"log_carat\", yaxis1_title=\"price\", xaxis3_title=\"carat²\", yaxis3_title=\"price\", \n",
    "                  height=800, width=1250, coloraxis={'colorscale': 'Turbo', 'cmin': -1, 'cmax': 1}, \n",
    "                  title= 'Análise exploratória entre as features criadas e o preço')\n",
    "\n",
    "fig.show('png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5227a",
   "metadata": {},
   "source": [
    "- Acima foram feitas as transformações da variável `carat` por meio da transformação logarítmica e quadrática, gerando duas novas variáveis. Considerando esse contexto, faz sentido que essas variáveis apresentem relações não lineares (como observado nos gráficos de dispersão). Por esse motivo, foi aplicada a correlação de Spearman no heatmap, uma vez que ela é adequada para capturar relações monotônicas, que podem ser não lineares. Diferentemente da correlação de Pearson, que assume uma relação linear e exige que ambas as variáveis possuam distribuição aproximadamente normal.\n",
    "\n",
    "- A respeito das correlações, observa-se uma correlação positiva, monotônica e alta para ambas as variáveis, assim como foi observado anteriormente com a feature volume. Como ambas as variáveis são transformações diferentes da variável `carat` em escalas distintas, seus valores numéricos no gráfico de dispersão são diferentes, mas a correlação permanece praticamente a mesma, já que ambas medem essencialmente a mesma informação. \n",
    "\n",
    "- Portanto, seria interessante considerar uma ou outra na próxima etapa de modelagem preditiva, evitando a inclusão simultânea devido à alta multicolinearidade entre elas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e76c07",
   "metadata": {},
   "source": [
    "## 2. Classificação Ordinal Multi-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58328ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'A652.pickle'\n",
    "\n",
    "f = open(filename, 'rb')\n",
    "\n",
    "(X_train, y_train, X_val, y_val, X_test, y_test) = pickle.load(f)\n",
    "print(\"Shapes: \", X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a45676",
   "metadata": {},
   "source": [
    "### Configurações experimentais (Pré-processamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para transformar os targets númericos y_* em categórico de acordo com os ranges do enunciado\n",
    "def numerical_to_categorical(y):\n",
    "    \"\"\"    \n",
    "    Função para categorizar as faixas de valores do target númerico\n",
    "    \n",
    "    0           -> 'None'\n",
    "    (0, 5]      -> 'Weak'\n",
    "    (5, 25]     -> 'Moderate'\n",
    "    (25, 50]    -> 'Strong'\n",
    "    (50, +inf]  -> 'Extreme'\n",
    "    \"\"\"\n",
    "\n",
    "    conditions = [\n",
    "        y == 0,\n",
    "        (y > 0) & (y <= 5),\n",
    "        (y > 5) & (y <= 25),\n",
    "        (y > 25) & (y <= 50),\n",
    "        y > 50\n",
    "    ]\n",
    "\n",
    "    choices = ['None', 'Weak', 'Moderate', 'Strong', 'Extreme']\n",
    "\n",
    "    return np.select(conditions, choices, default='Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformação dos ranges númericos dos targets de treino, validação e teste em categorias\n",
    "y_train_cat = numerical_to_categorical(y_train.ravel())\n",
    "y_test_cat = numerical_to_categorical(y_test.ravel())\n",
    "\n",
    "print('y_train:\\n', pd.Series(y_train_cat).value_counts())\n",
    "print('\\ny_test:\\n', pd.Series(y_test_cat).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cat = np.where(y_test_cat == 'Extreme', 'Strong', y_test_cat)\n",
    "print('\\ny_test:\\n', pd.Series(y_test_cat).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94359f5",
   "metadata": {},
   "source": [
    "- Como não há a classe 'Extreme' no conjunto de treino, isso pode gerar problemas na etapa de predição multiclasse ordinal, pois existe um ponto em que é criado um target para comparar 'Target > Classe?' e isso irá gerar um target apenas constituído por zero a partir do 'Strong', pois não há nenhum valor acima dele igual a 1 no conjunto de treino, dando erro no algoritmo do GradientBoostClassifier. Logo, para evitar esse problema, substitui o 'Extreme' para a classe 'Strong'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85411787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a ordem das categorias e aplica o OrdinalEncoder para codificar as categorias do targets em um númerica ordinal\n",
    "# O OrdinalEnconder foi aplicado de uma forma que evitasse DataLeakage\n",
    "categories_order = [['None', 'Weak', 'Moderate', 'Strong', 'Extreme']]\n",
    "encoder = OrdinalEncoder(categories=categories_order)\n",
    "\n",
    "y_train_ord = pd.Series(encoder.fit_transform(y_train_cat.reshape(-1, 1)).astype(int).ravel())\n",
    "y_test_ord = pd.Series(encoder.transform(y_test_cat.reshape(-1, 1)).astype(int).ravel())\n",
    "\n",
    "print('y_train:\\n', y_train_ord.value_counts())\n",
    "print('\\ny_test:\\n', y_test_ord.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa259d",
   "metadata": {},
   "source": [
    "- Podemos observar que a classe majoritária nas duas porções é a 'None' (classe 0), o que significa que a maior parte dos dados representa momentos sem chuva. O desbalanceamento é evidente e pode prejudicar a performance dos modelos de Machine Learning.\n",
    "\n",
    "- Outro ponto a se notar é que a categoria 'Extreme' (classe 4) é tão rara que não possui registros nas porções de treino, aparecendo apenas duas vezes no conjunto de teste. Isso indica que o modelo provavelmente não terá a oportunidade de aprender sobre essa classe, sendo assim, tenderá a \"chutar\" um valor para ela."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63126d",
   "metadata": {},
   "source": [
    "### Modelagem preditiva Baseline (Multiclassificação normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a8111",
   "metadata": {},
   "source": [
    "#### Treinamento e predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_class_report(y_test, y_pred, labels=None, target_names=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Função para obter a matriz de confusão e classification_report\n",
    "\n",
    "    \"\"\"\n",
    "    #Confusion metrics of the model built on down-sampled data\n",
    "    conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=labels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax.matshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', color=\"white\" if conf_matrix[i, j] > conf_matrix.max()/2 else \"black\")\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Actuals', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test, y_pred, labels=labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b60ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo\n",
    "model = GradientBoostingClassifier(random_state=seed)\n",
    "model.fit(X_train, y_train_ord)\n",
    "\n",
    "# Teste\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "y_pred = encoder.inverse_transform(y_pred_test.reshape(-1, 1)).ravel()\n",
    "y_true = encoder.inverse_transform(y_test_ord.to_numpy().reshape(-1, 1)).ravel()\n",
    "\n",
    "labels = ['None', 'Weak', 'Moderate', 'Strong']\n",
    "plot_confusion_matrix_class_report(y_true, y_pred, labels=labels, target_names=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebef2a50",
   "metadata": {},
   "source": [
    "#### Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1177f",
   "metadata": {},
   "source": [
    "- A partir da modelagem baseline de classificação multiclasse, considerando o target como uma variável categórica nominal, o GradientBoostingClassifier adota uma metodologia interna que permite o treinamento com um target não binário. A partir disso, observamos que a acurácia resultante foi bastante alta. No entanto, essa métrica não pode ser considerada confiável, uma vez que há um desbalanceamento acentuado entre as classes, o que favorece o modelo a acertar frequentemente ao simplesmente prever a classe 0. Para uma avaliação mais robusta, será utilizado principalmente o f1-score por classe.\n",
    "\n",
    "- Considerando que o objetivo é prever se haverá chuva, as classes mais relevantes a serem previstas são da 1 à 4. Essas classes apresentam, teoricamente, uma ordem de importância a ser considerada, sendo cada vez mais interessante prever corretamente as classes superiores, pois representam eventos mais raros. Porém, para essa modelagem baseline isso não foi considerado. No entanto, ess\n",
    "\n",
    "    Como observado anteriormente, a raridade de classes implica em menos amostras disponíveis para o treinamento do modelo. Logo, o conjunto de treinamento possuiu apenas dois exemplos para a classe 3 e nenhum exemplo para a classe 4, o que justifica o f1-score de 0% para essas duas classes mais raras, pois o modelo não teve informações suficientes (ou nenhuma) para aprender a predizê-las, sofrendo assim com underfitting e um grande desbalanceamento. O mesmo ocorreu em menor grau com a classe 2, que contou com poucos exemplos no treino e teste, resultando em um desempenho igualmente baixo e quase nulo.\n",
    "\n",
    "    Por fim, a segunda melhor classe em desempenho foi a classe 1, a qual apresentou a segunda maior quantidade de exemplos. O modelo pôde realizar um treinamento mais adequado sobre essa classe, alcançando um F1-score de 41%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d179002",
   "metadata": {},
   "source": [
    "### Modelagem preditiva principal (Multiclassificação ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bcf0d",
   "metadata": {},
   "source": [
    "#### Treinamento e predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ordinal_proba(X, models):\n",
    "    \"\"\"\n",
    "    Combina as previsões dos modelos binários para estimar as probabilidades de cada classe ordinal.\n",
    "    \"\"\"\n",
    "    k = len(models) + 1  # número total de classes\n",
    "    n = X.shape[0]       # número de instâncias\n",
    "    probas = np.zeros((n, k))  # matriz de probabilidades finais\n",
    "\n",
    "    # Obtemos P(y > i) de cada classificador binário\n",
    "    prob_gt = [model.predict_proba(X)[:, 1] for model in models]\n",
    "\n",
    "    # Probabilidade da primeira classe (ex: 'None') é: 1 - P(y > 0)\n",
    "    probas[:, 0] = 1 - prob_gt[0]\n",
    "\n",
    "    # Classes intermediárias: P(y = i) = P(y > i−1) * (1 − P(y > i))\n",
    "    for i in range(1, k - 1):\n",
    "        probas[:, i] = prob_gt[i - 1] * (1 - prob_gt[i])\n",
    "\n",
    "    # Última classe é simplesmente P(y > k−2), pois é teoricamente o restante\n",
    "    probas[:, k - 1] = prob_gt[-1]\n",
    "\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e152574",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = y_train_ord.nunique()\n",
    "models = []\n",
    "\n",
    "# Para cada limiar i (i = 0 até k−2), criamos um modelo que responde: \"Classe > i?\"\n",
    "for i in range(k - 1):\n",
    "    print('Target index:', i)\n",
    "    print('Valores por classe:\\n', y_train_ord.value_counts())\n",
    "    y_binary = (y_train_ord > i).astype(int)  # binariza o target: 1 se classe > i, senão 0\n",
    "\n",
    "    print('Classe > i?:\\n', y_binary.value_counts())\n",
    "    clf = GradientBoostingClassifier(random_state=seed)\n",
    "    clf.fit(X_train, y_binary)  # Treina modelo binário para esse limiar\n",
    "    models.append(clf)\n",
    "    print('--------------\\n')\n",
    "\n",
    "# Estima as probabilidades finais por classe\n",
    "probas_test = predict_ordinal_proba(X_test, models)\n",
    "\n",
    "# A classe final é aquela com maior probabilidade (argmax)\n",
    "y_pred_ord = np.argmax(probas_test, axis=1)\n",
    "\n",
    "# Decodifica os rótulos numéricos de volta para nomes de classe\n",
    "y_pred_labels = encoder.inverse_transform(y_pred_ord.reshape(-1, 1)).ravel()\n",
    "y_true_labels = encoder.inverse_transform(y_test_ord.to_numpy().reshape(-1, 1)).ravel()\n",
    "\n",
    "labels = ['None', 'Weak', 'Moderate', 'Strong']\n",
    "plot_confusion_matrix_class_report(y_true_labels, y_pred_labels, labels=labels, target_names=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af0018",
   "metadata": {},
   "source": [
    "#### Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67da6a",
   "metadata": {},
   "source": [
    "- Por meio da metodologia de classificação ordinal multiclasse abordada no artigo, foi realizada a modelagem para cada dataset associado a uma tarefa binária do tipo \"Classe > i?\", sendo atribuído o valor 1 nos casos em que a classe está acima na hierarquia ordinal, e 0 caso contrário. Esse processo foi repetido para cada uma das classes, exceto para a última (no caso, 'Strong') pois não há valores superiores a ela, então não faria sentido. A partir dessa modelagem, foi possível estimar as probabilidades para cada uma das classes, sendo escolhida como classificação final aquela com maior probabilidade entre todas, para cada exemplo no conjunto de teste.\n",
    "\n",
    "- Em relação aos resultados metrificados pelo f1-score principalmente, observa-se que, assim como na abordagem tradicional (baseline), o desbalanceamento dos dados continua sendo um problema predominante, fazendo com que a classe 'None' tenha desempenho significativamente superior, enquanto as demais apresentem métricas mais baixas, como é o caso da classe 'Strong', que teve apenas 2 exemplos no conjunto de treinamento.\n",
    "\n",
    "    Considerando o respeito à ordem entre as classes proporcionado por essa metodologia alternativa, percebe-se que, apesar do desbalanceamento favorecer classificações para a classe majoritária, houve uma melhora no f1-score da classe 'Weak', passando de 41% para 46% (um aumento de 5%), e da classe 'Moderate', de 6% para 7% (um aumento de 1%). Esses resultados indicam que a segunda abordagem é útil para capturar nuances nos dados ordinais que não foram percebidas no baseline, devido ao fato de este tratar o target como se fosse uma variável nominal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ef353",
   "metadata": {},
   "source": [
    "## 3. SHAP Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6daa7",
   "metadata": {},
   "source": [
    "## 4. Redução de Dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559aea82",
   "metadata": {},
   "source": [
    "## 5. Predição Conforme"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
